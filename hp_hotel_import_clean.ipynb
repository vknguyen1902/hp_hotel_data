{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\khanhdi\\anaconda3\\envs\\python36\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean Hotel Dataset \n",
    "\n",
    "Our dataset is in Vietnamese and has many irrelevant rows and columns. We are going to have a function that takes care of the importing and cleaning data to make the dataset ready for analysis. The function below will take input and produce output as explained below:\n",
    "\n",
    "### Input:\n",
    "+ inputfile (type: str): path to Excel file\n",
    "+ sheetname (type: str): sheet to be cleaned by the function\n",
    "\n",
    "### Output: \n",
    "+ type: None\n",
    "+ Post-condition: all the empty and extraneous rows will be dropped and \n",
    "    the resulting data will be saved to a new CSV file\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hs_cleaner(inputfile, sheetname):\n",
    "    xl = pd.ExcelFile(inputfile)\n",
    "    df = xl.parse(sheetname)\n",
    "\n",
    "    # Skip the first 4 lines (file title and empty lines)\n",
    "    df = df[4:] \n",
    "\n",
    "    df.columns = [\"id\", \"bill_number\", \"room_number\", \"room_type\", \"checkin\", \"checkin_time\", \\\n",
    "                    \"checkin_date\", \"checkout\", \"checkout_time\", \"checkout_date\", \"by_day\", \"by_hour\", \\\n",
    "                    \"by_night\", \"room_price\", \"service_fee\", \"laundry_fee\", \"phone_card_fee\", \"vat\", \\\n",
    "                    \"other_fee\", \"other_discount\", \"total_fee\", \"paid_before\", \"ppaid_before\", \"credit\", \\\n",
    "                    \"credit_payment\", \"expense\", \"actual_fee_payment\", \"note\", \"random\", \"num_night_check_in\", \\\n",
    "                    \"num_morning_check_in\", \"num_by_hour\", \"num_by_night\", \"\"]\n",
    "\n",
    "    trash = [] # store all the row index to be dropped\n",
    "    trashline = 0 # store the last line to read\n",
    "    num_rows = df.shape[0]\n",
    "    # Loop over each row, store the indices to be removed\n",
    "    for i in range(num_rows):\n",
    "        # Everything after this row will be dropped\n",
    "        if df.iloc[i][\"id\"] == \"TỔNG HỢP NỢ, TRẢ, TRẢ TRƯỚC, TRỪ TRẢ TRƯỚC TRONG NGÀY\":\n",
    "            trashline = i\n",
    "            break\n",
    "\n",
    "        # If row doesn't have bill number, there is no transaction going on \n",
    "        # so it should be dropped too\n",
    "        if df.iloc[i][\"bill_number\"] is np.nan:\n",
    "            trash.append(i)        \n",
    "\n",
    "    # Drop the trashy rows\n",
    "    print(\"Rows to be dropped:\", trash, \"and\", trashline)\n",
    "    df = df.drop(df.index[[j for j in range(trashline, num_rows)]])\n",
    "    df = df.drop(df.index[trash])\n",
    "\n",
    "    # Save dataframe to csv\n",
    "    filename = inputfile[:-5] + \"_cleaned\" + \".csv\"\n",
    "    df.to_csv(filename, sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\khanhdi'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function by importing the first fifteen days of March 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows to be dropped: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 52, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77] and 78\n",
      "Rows to be dropped: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 60, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87] and 88\n",
      "Rows to be dropped: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 30, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88] and 89\n",
      "Rows to be dropped: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 28, 29, 30, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 61, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85] and 86\n",
      "Rows to be dropped: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 58, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84] and 85\n",
      "Rows to be dropped: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89] and 90\n",
      "Rows to be dropped: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 58, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84] and 85\n",
      "Rows to be dropped: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85] and 86\n",
      "Rows to be dropped: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 60, 66, 67, 68, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92] and 93\n",
      "Rows to be dropped: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85] and 86\n",
      "Rows to be dropped: [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85] and 86\n",
      "Rows to be dropped: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 53, 54, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82] and 83\n",
      "Rows to be dropped: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 63, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88] and 89\n",
      "Rows to be dropped: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 58, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85] and 86\n",
      "Rows to be dropped: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84] and 85\n"
     ]
    }
   ],
   "source": [
    "#hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 010318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 020318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 030318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 040318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 050318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 060318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 070318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 080318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 090318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 100318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 110318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 110318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 120318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 130318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 140318.xlsx', \"THHP\")\n",
    "hs_cleaner('C:\\\\Users\\\\khanhdi\\\\HP 150318.xlsx', \"THHP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
